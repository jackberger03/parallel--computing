#!/bin/bash
##ENVIRONMENT SETTINGS; CHANGE WITH CAUTION
#SBATCH --export=NONE            #Do not propagate environment
#SBATCH --get-user-env=L         #Replicate login environment
#
##NECESSARY JOB SPECIFICATIONS
#SBATCH --job-name=MPI_Exp6_N8      #Set the job name
#SBATCH --time=2:00:00           #Set the wall clock limit (adjust if n=10^10 takes longer)
#SBATCH --nodes=8               #Request calculated nodes
#SBATCH --ntasks-per-node=8      #Request specific tasks/cores per node
#SBATCH --mem=8G                 #Request 8GB per node (total memory for tasks on that node)
#SBATCH --output=mpi_exp6_ntasks8_output.%j #Send stdout/err to output file
#
##OPTIONAL JOB SPECIFICATIONS - IMPORTANT: UPDATE THESE FOR YOUR GRACE ACCOUNT DETAILS
##SBATCH --account=your_account_here    #Replace with your account name
##SBATCH --partition=your_partition_here #Replace with your partition (e.g., shared, compute)
##SBATCH --qos=your_qos_here            #Replace with your QOS (e.g., normal, short)
#
##First Executable Line
#
module load intel               # Load Intel software stack (adjust for your cluster if needed, e.g., intel/2023.1 impi/2021.9)

# Compile MPI program - this ensures the .exe is built right before execution.
# If you compiled it once manually via calculate.sh or command line, you can comment this out.
mpiicx -o compute_pi_mpi.exe compute_pi_mpi.c

echo "=== Experiment 6: n=10000000000, p=64 ==="
echo "SLURM Configuration: Nodes = 8, Ntasks-per-node = 8"
echo "Ntasks-per-node = 8" # This line is for easy parsing of the output file by main.py

# Run the MPI program. Using srun with --mpi=pmix is common for Intel MPI on SLURM.
# SLURM will use the #SBATCH directives to place the 64 tasks.
srun --mpi=pmix -n 64 ./compute_pi_mpi.exe 10000000000

echo "=== Experiment 6 (Ntasks-per-node = 8) Complete ==="
